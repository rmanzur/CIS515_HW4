\documentclass[12pt]{article}
\usepackage{amsfonts,amssymb,amsmath}
%\documentstyle[12pt,amsfonts]{article}
%\documentstyle{article}

\setlength{\topmargin}{-.5in}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\textwidth}{6.5truein}
\setlength{\textheight}{8.5truein}
\setcounter{MaxMatrixCols}{15}
%\input ../basicmath/basicmathmac.tex
%
%\input ../adgeomcs/lamacb.tex
\input mac-new.tex
\input mathmac-v2.tex
%\input ../adgeomcs/mac.tex
%\input ../adgeomcs/mathmac.tex

\def\fseq#1#2{(#1_{#2})_{#2\geq 1}}
\def\fsseq#1#2#3{(#1_{#3(#2)})_{#2\geq 1}}
\def\qleq{\sqsubseteq}

%
\begin{document}
\begin{center}
\fbox{{\Large\bf Fall,  2016 \hspace*{0.4cm} CIS 515}}\\
\vspace{1cm}
{\Large\bf Fundamentals of Linear Algebra and Optimization\\
Jean Gallier \\
\vspace{0.5cm}
Homework 4}\\[10pt]
October 27, 2016; Due November 8, 2016, beginning of class\\
\end{center}


\vspace {0.25cm}\noindent
{\bf Problem B1 (50 pts).} \\
\medskip
(1)Let's define matrix A as : $\begin{pmatrix}
a_{11} & a_{12} \\a_{21} & a_{22}
\end{pmatrix}$ Then, we get: $a_{11} + a_{12} = c_1 = a_{21} + a_{22}$ and $a_{11} + a_{21} = c_2 = a_{12} + a_{22}$. We can rewrite the equations as:
\begin{equation}
a_{11} + a_{12} - a_{21} -a_{22} = 0
\end{equation}
\begin{equation}
a_{11} + a_{21} - a_{12} - a_{22} = 0
\end{equation}
From part d of the duality theorem (3.14), we know that $$dim(U) + dim(U^0) = dim(E)$$
$dim(E)$ is $n^2$ or 4 and $dim(U)$ is $2(n-1)$ or 2 since there are two linearly indepedent equations. Then, $dim(U^0)$ is $4 - 2 = 2$.  \\
\medskip
(2)
The equations corresponding to the three conditions listed (sum of entries in each row are the same, sum of entries in each column are the same, $c_1 = c_2$) are the following:
\begin{equation}
a_{11} + a_{12} - a_{21} - a_{22} = 0
\end{equation}
\begin{equation}
a_{11} + a_{21} - a_{12} - a_{22} = 0
\end{equation}
\begin{equation}
a_{12} - a_{21}  = 0
\end{equation}
\begin{equation}
a_{11} - a_{22} = 0
\end{equation}
\begin{equation}
a_{22} - a_{11} = 0
\end{equation}
\begin{equation}
a_{21} - a_{12} = 0
\end{equation}

It is clear that only the first two equations are linearly independent so $dim(U)=2$ and the dimension of the subspace of $2 \times 2$ matrices is still 2 as it is in section (1) of this problem.

From equations (5) and (6), we get that every such matrix is of the form $\begin{pmatrix}
a & b \\ b& a
\end{pmatrix}$ and a basis for the subspace consists of 4 matrices which are $\begin{pmatrix}
a & 0 \\ 0& 0
\end{pmatrix}$ $\begin{pmatrix}
0 & b \\ 0& 0
\end{pmatrix}$
$\begin{pmatrix}
0 & 0 \\ b& 0
\end{pmatrix}$
$\begin{pmatrix}
0 & 0 \\ 0& a
\end{pmatrix}$



\medskip
(3)
Using the duality theorem again, we get: $n^2 - 2(n -1) = 9 - 2(3 -1) = 5$ as the dimension of the subspace of the matrices A. The set of equations are given as 

\[
\begin{pmatrix}
1 & 1 &  1 &  -1 & -1  & -1 & 0 & 0 & 0\\
0  &  0  &  0 & 1 & 1 &  1 &  -1 & -1  & -1  \\
1  &  -1  &  0 & 1 & -1  &  0 &  1 &  -1  &  0 \\
0  &  1  &  -1 & 0 &  1  &  -1 &  0 &  1  &  -1 \\
0  &  1  &  1 & -1 &  0  &  0 &  -1 &  0  &  0 \\
\end{pmatrix}
\begin{pmatrix}
a_{1 1} \\
a_{1 2} \\
a_{1 3} \\
a_{2 1} \\
a_{2 2 }  \\
a_{2 3 }  \\
a_{3 1 }  \\
a_{3 2 }  \\
a_{3 3 }  \\
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0\\
0\\
0
\end{pmatrix} .
\]
For example, by looking at the equation corresponding to the first row, we have: $a_{11} + a_{12} + a_{13} = c_1$ and $a_{21} + a_{22} + a_{23} = c_1$ so $a_{11} + a_{12} + a_{13} -a_{21} - a_{22} - a_{23} = 0$. The same reasoning applies for all 5 of the equations represented by the matrix. \\In rref , the matrix becomes:
\[ 
\begin{pmatrix}
1 & 0 &  0 &  0 & -1  & -1 & 1 & 0 & 0\\
0  &  1  &  0 & 0 & 1 &  0 &  -1 & 0  & -1  \\
0  &  0  &  1 & 0 & 0  &  1 &  -1 &  -1  &  0 \\
0  &  0  &  0 & 1 &  1  &  1 &  -1 &  -1  &  -1 \\
0  &  0  &  0 & 0 &  0  &  0 &  0 &  0  &  0 \\
\end{pmatrix}\] which shows 4 linearly independent equations corresponding to the $2(n-1)$ term in $n^2 - 2(n -1)$.
We can find a basis for the kernel of this matrix by applying the algorithm from the notes. So then,
$$BK = \begin {pmatrix} 1 & 1 & -1 & 0 & 0\\ -1 & 0 & 1 & 0 & 1\\ 0 & -1 & 1 & 1 & 0\\ -1 & -1 & 1 & 1 & 1\\1 & 0 & 0 & 0 & 0\\
 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 1 & 0\\  0 & 0 & 0 & 0 & 1 \end{pmatrix}$$


We arrive at 5 matrices that form the basis which are:
$$M_1 = \begin{pmatrix} 1 & -1 & 0 \\ -1 & 1 & 0\\ 0 & 0 & 0 \end{pmatrix} \\
M_2 = \begin{pmatrix} 1 & 0 & -1 \\ -1 & 0 & 1\\ 0 & 0 & 0 \end{pmatrix} \\
M_3 = \begin{pmatrix} -1 & 1 & 1 \\ 1 & 0 & 0\\ 0 & 0 & 0 \end{pmatrix}\\
M_4 = \begin{pmatrix} 0 & 0 & 1 \\ 1 & 0 & 0\\ 0 & 0 & 0 \end{pmatrix}\\
M_5 = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0\\ 0 & 0 & 0 \end{pmatrix}\\
$$


\vspace {0.25cm}\noindent
{\bf Problem B2 (10 pts).} \\
Because $A$ is symmetric and positive definite then $\forall x \neq 0, \; x^\top (B^\top A B) x = (Bx)^\top A (Bx)$, since $B$ is invertible then $Bx \neq 0$, thus it comes $(Bx)^\top A (Bx) > 0$, which means $B^\top A B$ is also positive definite.\\
Conversely, when $B^\top A B$ is positive definite, then it can be written as $B^\top A B = D D^\top \Rightarrow A = (B^\top)^{-1} D ((B^\top)^{-1} D )^\top$. Since $B$ is invertible then $ (B^\top)^{-1} D$ is also invertible  ,which means that $A$ is positive definite.

\vspace {0.25cm}\noindent
{\bf Problem B3 (100 pts).}\\
(1) 
$$ S A A^{-1} = \begin{pmatrix}
1 & 0 \\ 0 & ad - bc
\end{pmatrix}A^{-1}$$
$$ S = \begin{pmatrix}
1 & 0 \\ 0 & ad - bc
\end{pmatrix}A^{-1}$$
To find $A^{-1}$, we use Gauss-Jordan elimination on $\begin{pmatrix}
a & b \\ c & d
\end{pmatrix}$.
We perform the following row operations:\\
1) $R_1/a \rightarrow R_1$\\
2) $-cR_1 + R_2 \rightarrow R_2$\\
3) $R_2/(d - \frac{cb}{a}) $\\
4) $-\frac{b}{a}R_2 + R_1 \rightarrow R_1$\\ 
So $A^{-1} = \begin{pmatrix}
\frac{d}{ad - bc} & \frac{-b}{ad - bc}\\
\frac{-c}{ad - bc} & \frac{a}{ad - bc}
\end{pmatrix}$ and $S = \begin{pmatrix}
\frac{d}{ad - bc} & \frac{-b}{ad - bc}\\
-c & a
\end{pmatrix}$
\\ 
Indeed $S$ is the product of at most 4 elementary matrices as we find that:
$$E_{12;\frac{-b}{d}}E_{22;(\frac{bc}{d} +a})E_{21;c}E_{11;\frac{ad - bc}{d}}S = I$$
Then $$ S = E^{-1}_{12;\frac{-b}{d}}E^{-1}_{22;(\frac{bc}{d} +a})E^{-1}_{21;c}E^{-1}_{11;\frac{ad - bc}{d}}$$
For $A = \begin{pmatrix}
a & 0 \\ 0 & a^{-1}
\end{pmatrix}$ the row operations to get the identity matrix are the following:\\
1. $R_1/a \rightarrow R_1$\\
2. $R_2/a^{-1} \rightarrow R_2 $\\

so $E_{22; a^{-1}} E_{11; a} A = I $ and 
$A = E^{-1}_{11; a}E^{-1}_{22; a^{-1}}$.\\
When $a = -1$, the decomposition becomes:
$$\begin{pmatrix}
-1 & 0 \\ 0 & 1
\end{pmatrix}\begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix} = A$$

\medskip
(2)
$$R = \begin{pmatrix}
1 & u \\ 0 & 1
\end{pmatrix} \begin{pmatrix}
1 & 0 \\ v & 1
\end{pmatrix}\begin{pmatrix}
1 & u \\ 0 & 1
\end{pmatrix} = \begin{pmatrix}
cos \theta & -sin \theta \\ sin \theta & cos \theta
\end{pmatrix}$$
$$ = \begin{pmatrix}
uv +1 & 2u + u^2v \\ v & uv +1
\end{pmatrix}= \begin{pmatrix}
cos \theta & -sin \theta \\ sin \theta & cos \theta
\end{pmatrix}$$
So then it is clear that $v = sin\theta$ and $u = \frac{cos\theta - 1}{sin \theta} = -tan \frac{\theta}{2}$

\medskip
(3)\\
\medskip
(a)
Write $A = \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_i \\
\vdots \\
\alpha_j \\
\vdots \\
\alpha_n
\end{pmatrix}$, $E = \begin{pmatrix}
e_1 \\
\vdots \\
e_i \\
\vdots \\
e_j \\
\vdots \\
e_n
\end{pmatrix}$, where $e_i$ is the basis vector. \\
Choose $E_1(i,j) = \begin{pmatrix}
e_1 \\
\vdots \\
e_i \\
\vdots \\
e_j - e_i \\
\vdots \\
e_n
\end{pmatrix}$, then $A_1 = E_1(i,j) A =  \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_i \\
\vdots \\
\alpha_j - \alpha_i\\
\vdots \\
\alpha_n
\end{pmatrix}$.
Choose $E_2(i,j) = \begin{pmatrix}
e_1 \\
\vdots \\
e_i + e_j\\
\vdots \\
e_j  \\
\vdots \\
e_n
\end{pmatrix}$, then $A_2 = E_2(i,j) A_1 = \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_j \\
\vdots \\
\alpha_j - \alpha_i\\
\vdots \\
\alpha_n
\end{pmatrix}$. And 
$A_3 = E_1(i,j) A_2 = \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_j \\
\vdots \\
- \alpha_i\\
\vdots \\
\alpha_n
\end{pmatrix}$,
$A_4 = E_{j,-1} A_3 = \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_j \\
\vdots \\
\alpha_i\\
\vdots \\
\alpha_n
\end{pmatrix}$, thus $E_{j,-1} E_1(i,j)E_2(i,j)E_1(i,j) A = P(i, j) A$. \\
Since $E_{j,-1} E_{j,-1} = I$, multiply $E_{j,-1}$ on both sides, we get $E_1(i,j)E_2(i,j)E_1(i,j) A = E_{j,-1}P(i, j) A$. \\
\medskip
(b)
Write $
E_{n,d} = \begin{pmatrix}
e_1 \\
\vdots \\
d e_n
\end{pmatrix}
$, choose $
E_3(i,d) = \begin{pmatrix}
e_1 \\
\vdots \\
e_i - \frac{1-d}{d^2} e_n \\
\vdots \\
e_n
\end{pmatrix}
$, $
E_4(i,d) = \begin{pmatrix}
e_1 \\
\vdots \\
e_i \\
\vdots \\
e_n - de_i
\end{pmatrix}
$, $
E_5(i,d) = \begin{pmatrix}
e_1 \\
\vdots \\
e_i + \frac{1-d}{d}e_n \\
\vdots \\
e_n 
\end{pmatrix}
$, $
E_6(i,d) = \begin{pmatrix}
e_1 \\
\vdots \\
e_i \\
\vdots \\
e_n + e_i
\end{pmatrix}
$. Then we can easily find that 
\begin{align*}
E_{n,d}^{(1)} &= E_3(i,d)E_{n,d} = \begin{pmatrix}
e_1 \\
\vdots \\
e_i - \frac{1-d}{d}e_n\\
\vdots \\
de_n 
\end{pmatrix} \\
E_{n,d}^{(2)} &= E_4(i,d)E_{n,d}^{(1)} = \begin{pmatrix}
e_1 \\
\vdots \\
e_i - \frac{1-d}{d}e_n\\
\vdots \\
-de_i + e_n 
\end{pmatrix} \\
E_{n,d}^{(3)} &= E_5(i,d)E_{n,d}^{(2)} = \begin{pmatrix}
e_1 \\
\vdots \\
de_i \\
\vdots \\
-de_i + e_n 
\end{pmatrix} \\
E_{n,d}^{(4)} &= E_6(i,d)E_{n,d}^{(3)} = \begin{pmatrix}
e_1 \\
\vdots \\
de_i \\
\vdots \\
e_n 
\end{pmatrix} \\
\end{align*}
As a conclusion, $E_6(i,d)E_5(i,d)E_4(i,d)E_3(i,d) E_{n,d} = E_{i, d}$.\\
When $d = -1$, $E_3(i,d) = E_5(i,d) \; E_4(i,d) = E_6(i,d)$. \\
\medskip
(c)
We can see that all the permutation matrix can be written as $P = \sum_{i,j} P(i,j)$, and from above we prove that $P(i,j)$ can be written as the product of elementary operation of the form $E_{k,\ell; \beta}$ and 
$E_{n, -1}$, such that $E_{j,-1} E_1(i,j)E_2(i,j)E_1(i,j) = P(i, j) $, so it is true for all permutation matrices.

\medskip
(4)
First, we can use the Gaussion elimination, such that a series of elementary matrices $(\prod_{i} E_{k,\ell; \beta}^{i}) A = U$. Since $A$ is invertible, we get $\prod_i u_{ii} \neq 0$, use $E_{i, u_{ii}^{-1}}$ to $U$, then we can get an upper-triangle matrix whose diagonal entries (except the last row) are $1$, such that $(\prod_i E_{i, u_{ii}^{-1}})(\prod_{(i)} E_{k,\ell; \beta}^{i}) A = \begin{pmatrix}
1 & a_{12} & \cdots & a_{1n} \\
0 & 1 &\cdots & \vdots \\
\vdots & \cdots & \cdots & \vdots \\ 
0 & 0 & \cdots & a_{nn} \\ 
\end{pmatrix}$. Then apply a series of operations to it to make $a_{ij} = 0, \forall j > i, i \neq n$, such that $SA = (\prod_{i} E_{k,\ell; \beta}^{*(i)})(\prod_i E_{i, u_{ii}^{-1}})(\prod_{(i)} E_{k,\ell; \beta}^{i}) A  = \begin{pmatrix}
I_{n - 1} & 0 \\
0 & d
\end{pmatrix}$, since $\det(S) \det(A) = a_{nn} \Rightarrow d = \det(A)$.\\
Write $A = \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_n
\end{pmatrix}$ we can use induction, \\
when $n = 1$, we can find an entry in the first column such that $a_{i1} \neq 0$, otherwise $\det(A) = 0$, which is a contradiction. And then we use $E_{1,i, \frac{a_{11} - 1}{a_{i1}}} = 
\begin{pmatrix}
e_1 - \frac{a_{11} - 1}{a_{i1}}e_i\\
\vdots \\
e_n 
\end{pmatrix}$, which makes the first entry $a_{11} = 1$, after a series operations such that $E_{1, j, a_{j1}} = \begin{pmatrix}
e_1 \\
\vdots \\
e_j - a_{j1}e_1 \\
\vdots \\
e_n 
\end{pmatrix}$, we can make the other entries $a_{i1} = 0\; \forall i \neq 1 $. These need $n$ steps, however if $a_{11}$ is the only non zero entry then we need another transportation matrix $P$ to permute the first row to some row in the first column, thus we need $n + 1$ matrices at most to get $A_1 = \begin{pmatrix}
1 & a_{12} & \cdots & a_{1n}\\
0 & a_{22} & \cdots & a_{2n}\\
\vdots & \cdots & \cdots & \vdots \\
0 & a_{n2} & \cdots & a_{nn}\\
\end{pmatrix}$. \\
It is the same when $n > 1$, while when we get the matrix $A_{n-1} = \begin{pmatrix}
1 & 0 & \cdots & a_{1n}\\
0 & 1 & \cdots & a_{2n}\\
\vdots & \cdots & \cdots & \vdots \\
0 & 0 & \cdots & a_{nn}\\
\end{pmatrix}$, because $A$ is invertible, comes the result that $a_{nn} = \det(A) = 1$, thus we only need $n-1$ matrices to get $A_{n} = I$. \\
As a conclusion, the total transformation is $(n+1)(n-1) + (n-1) = n(n+1) - 2$ at most.

\vspace {0.25cm}\noindent
{\bf Extra Credit (20 points).}
Prove that every matrix in $\mathbf{SL}(n)$
can be written as a product of at most
$(n  - 1)(\max\{n, 3\} + 1)$
elementary   matrices of the form  $E_{k,\ell; \beta}$.


\vspace {0.25cm}\noindent
{\bf Problem B4 (50 pts).} \\
We can use the induction to show that after $k$ round Gaussian elimination, such that $ A^{*} = \begin{pmatrix}
U & V \\
\mathbf{0} & A^{(k)}
\end{pmatrix}
$, $A^{(k)}$ is strictly column diagonally dominant and it does not require pivoting.\\
Suppose $A = \begin{pmatrix}
\alpha & u \\
v & B
\end{pmatrix}$,\\
When $k = 1$, there comes the Gaussian elimination which makes $A = \begin{pmatrix}
1 & 0 \\
\frac{v}{\alpha} & 1
\end{pmatrix} 
\begin{pmatrix}
1 & u \\
0 & B - \frac{uv}{\alpha}
\end{pmatrix}$. Therefore $A^{(1)} = B - \frac{uv}{\alpha}$.
We need to prove  $A^{(1)}$ is also strictly column diagonally dominant:
\[
|a_{j\, j}^{(1)}| > \sum_{i = 1,\, i\not= j}^{n} |a_{i\, j}^{(1)}|,
\quad\hbox{for $j = 1, \ldots, n$}
\]
Let $a_{ij}^{(1)}$ be the $i,j$ entry of $A^{(1)}$, then \begin{align*}
a_{ij}^{(1)} &= b_{ij} - \frac{u_{j}v_{i}}{\alpha} \\
\sum_{i\geq 2, i\neq j}|a_{ij}^{(1)}| &= \sum_{i\geq 2, i\neq j}|b_{ij} - \frac{u_{j}v_{i}}{\alpha}| \\
&\leq 
\sum_{i\geq 2, i\neq j}|b_{ij}| + \frac{|u_j|}{\alpha}\sum_{i\geq 2, i\neq j} |v_i|
\end{align*}
Since $A$ is strictly column diagonally dominant, it follows that 
\begin{align*}
\sum_{i\geq 2, i\neq j}|a_{ij}^{(1)}| & <
(|b_{jj}| - |u_{j}|) + \frac{|u_j|}{\alpha} (\alpha - |v_{j}|) \\
&= |b_{jj}| - \frac{|u_j|}{\alpha}|v_j| \\
&\leq |b_{jj} - \frac{u_j}{\alpha}v_j| \\
&= |a_{jj}^{(1)}|
\end{align*}
Thus $A^{(1)}$ is strictly column diagonally dominant, there is no need to pivot.
We can easily use the same way to prove that when $k > 1$, every round of Gaussian elimination leads to a 
strictly column diagonally dominant $A^{(k)}$. \\
Because $|a_{jj}| > \sum_{i = 1,\, i\not= j}^{n} |a_{i\, j}^{(1)}| \geq 0 \Rightarrow a_{jj} > 0$ and $\det(A) = \prod_i a_{ii}$, thus $A$ is invertible. \\




\vspace {0.25cm}\noindent
{\bf Problem B5 (40 pts).} \\
\medskip
(1)
Write the polynomial as $P(X) = a_0 + a_1 X + a_2 X^2 + \cdots a_m X^m$. Then from equations$P(\alpha_i) = \beta_i, \quad 1\leq i \leq m + 1. $, we can get the matrix form such that 
\begin{align*}
& A = 
\begin{pmatrix}
1 & \alpha_1^1 & \alpha_1^2 & \cdots & \alpha_m^m \\
1 & \alpha_2^1 & \alpha_2^2 & \cdots & \alpha_2^m \\
\vdots & \cdots & \cdots & \cdots & \vdots \\
1 & \alpha_{m+1}^1 & \alpha_{m+1}^2 & \cdots & \alpha_{m+1}^m \\
\end{pmatrix} \\
& X = 
\begin{pmatrix}
a_0 \\
a_1 \\
\vdots \\
a_m
\end{pmatrix} \\
& b = 
\begin{pmatrix}
\beta_1 \\
\beta_2  \\
\vdots \\
\beta_{m+1}
\end{pmatrix} \\
& AX = b
\end{align*}
Where we change $(a_i) \; 0 \leq i \leq m$ as our variables. Then consider $\det(A)$, think about the Vandermonde Matrix, we can easily get $\det(A) = \prod_{1 \leq i < j \leq m} (a_j - a_i)$. And since $(\alpha_1, \ldots, \alpha_{m + 1})$ is a sequence of pairwise distinct scalars, there comes $\det(A) \neq 0$, then we can get the unique solution of $X = A^{-1}b$, which means the coefficients of polynomial are unique, so $P$ is unique.

\medskip
(2) \\
(a) \\
If $i \neq j$, then $\prod_{k \neq i}(X - \alpha_k) = \prod_{k \neq i}(\alpha_j - \alpha_k) = 0 \Rightarrow L_i(\alpha_j) = 0$. \\
If $i = j$, then $\frac{\prod_{k \neq i}(X - \alpha_k)}{\prod_{k \neq i}(\alpha_i - \alpha_k)} = \frac{\prod_{k \neq i}(\alpha_i - \alpha_k)}{\prod_{k \neq i}(\alpha_i - \alpha_k)} = 1 \Rightarrow L_i(\alpha_j) = 1$.\\
Thus $L_i(\alpha_j) = \delta_{i\, j} \quad 1\leq i, j \leq m + 1$. \\
(b) \\
The degree of $L_i(X)$ is $m$ and $P(\alpha_i) = \sum_j \beta_j L_j(\alpha_i) = \sum_j \beta_j \delta_{i \, j} = \beta_i$. So the only thing we need to do is to prove the uniqueness of $P(X)$. \\
Suppose there is another polynomial $Q(X)$ satisfying our equations $P(\alpha_i) = \beta_i, \quad 1\leq i \leq m + 1$, then it must lead to the conclusion that $H(X) = Q(X) - P(X)$ is zero at $\alpha_i, \, 1 \leq i \leq m+1$. However, according to Fundamental Theorem of Algebra, $H(X)$ is at most $m$ degree and can has at most $m$ zero points unless it is identically zero polynomial. Therefore, $H(X)$ is the identically zero polynomial and $P(X) \equiv Q(X)$, which means that $P(X)$ is unique.

\medskip
(3)
Prove that $L_1(X), \dots, L_{m + 1}(X)$ are lineary independent, and that
they form a basis of all polynomials of degree at most $m$.

\medskip
How is $1$ (the constant polynomial $1$)
expressed over the basis $(L_1(X), \dots, L_{m + 1}(X))$?

\medskip
Give the expression of every polynomial $P(X)$ of degree at most $m$ over
the basis $(L_1(X), \dots, L_{m + 1}(X))$.

\medskip
(4)
Prove that the dual basis $(L_1^*, \dots, L_{m + 1}^*)$
of the basis $(L_1(X), \dots, L_{m + 1}(X))$ consists of the linear forms
$L_i^*$ given by
\[
L_i^*(P) = P(\alpha_i),
\]
for every polynomial $P$ of degree at most $m$;
this is simply {\it evaluation at $\alpha_i$\/}.

\vspace {0.25cm}\noindent
{\bf Problem B6 (60 pts).}
(a)
Find a lower triangular matrix $E$ such that
\[
E 
\begin{pmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 2 & 1 & 0 \\
1 & 3 & 3 & 1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & 1 & 2 & 1 
\end{pmatrix}.
\]
The lower triangular matrix E is:
\[ \begin{pmatrix}
1 & 0 & 0 & 0\\-1 & 1 & 0 & 0\\0 & -1 & 1 & 0\\0 & 0 & -1 & 1
\end{pmatrix}\]

We find E by observing that the row operations performed on matrix $Pa_3$ are (in the order listed below):\\
1. $R_1 \rightarrow R_1$\\
2. $-R_1 + R_2  \rightarrow R_2$\\
3. $-R_1 + R_3  \rightarrow R_3$\\
4. $-R_2 + R_3  \rightarrow R_3$\\
5. $-R_1 + R_4  \rightarrow R_4$\\
6. $-R_3 + R_4  \rightarrow R_4$\\
7. $-R_2 + R_4  \rightarrow R_4$\\
So we arrive at $E$ by multiplying the elementary matrices:
$$E_{42;-1}E_{43;-1}E_{41;-1}E_{32;-1}E_{31;-1}E_{21;-1}$$
\medskip
(b)
What is the effect of the product (on the left) with
\[
E_{4, 3; -1} E_{3, 2; -1} E_{4, 3; -1} E_{2, 1; -1} E_{3, 2; -1} E_{4, 3; -1}
\]
on the matrix 
\[
Pa_3 = 
\begin{pmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 2 & 1 & 0 \\
1 & 3 & 3 & 1
\end{pmatrix}.
\]
The product of the elementary matrices (on the left) on $Pa_3$ is the identity matrix: 
\[\begin{pmatrix}
1&0 &0& 0\\0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\0 & 0 & 0 & 1
\end{pmatrix}\]
\medskip
(c)
Find the inverse of the matrix $Pa_3$.
$$P^{-1}a_3Pa_3 = I$$

Thus, we use the following Gauss-Jordan elimination process in the order listed on $Pa_3$ and the identity matrix side by side:\\
1. Subtract $R_1$ from $R_2$, $R_3$ and $R_4$\\
2. Subtract $2 R_2$ from $R_3$\\
3. Subtract $3 R_2$ from $R_4$\\
4. Subtract $3 R_3$ from $R_4$\\
Then, $$P^{-1}a_3 = \begin{pmatrix}
1 & 0 & 0 & 0\\-1 & 1 & 0 & 0 \\1 & -2 & 1 & 0\\1 & 3 & -3 & 1
\end{pmatrix}$$ 
Furthermore,the inverse is equal to the product of the elementary matrices $
E_{4, 3; -1} E_{3, 2; -1} E_{4, 3; -1}\\ E_{2, 1; -1} E_{3, 2; -1} E_{4, 3; -1}
$from part (b).
\medskip
(d)
Consider the $(n+1)\times (n+1)$ Pascal matrix $Pa_{n}$
whose $i$th row is given by the binomial coefficients
\[
\binom{i-1}{j-1},
\]
with $1 \leq i \leq n + 1$, $1 \leq j \leq n + 1$,
and with the usual convention that
\[
\binom{0}{0} = 1, \quad \binom{i}{j} = 0 
\quad\hbox{if}\quad j > i.
\]
The matrix $Pa_3$ is shown in question (c) and $Pa_4$ is shown below:
\[
Pa_4 = 
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & 0 \\
1 & 2 & 1 & 0 & 0 \\
1 & 3 & 3 & 1 & 0 \\
1 & 4 & 6 & 4 & 1
\end{pmatrix}.
\]

Find $n$ elementary matrices $E_{i_k, j_k; \beta_k}$ such that
\[
E_{i_n, j_n; \beta_n} \cdots E_{i_1, j_1; \beta_1} Pa_n = 
\begin{pmatrix}
1 & 0 \\
0 & Pa_{n - 1}
\end{pmatrix}.
\]

Use the above to prove that the inverse of $Pa_n$ is the lower
triangular matrix whose $i$th row is given by the signed binomial
coefficients
\[
(-1)^{i + j - 2}\binom{i-1}{j-1},
\]
with $1 \leq i \leq n + 1$, $1 \leq j \leq n + 1$.
For example,
\[
Pa_4^{-1} = 
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
-1 & 1 & 0 & 0 & 0 \\
1 & -2 & 1 & 0 & 0 \\
-1 & 3 & -3 & 1 & 0 \\
1 & -4 & 6 & -4 & 1
\end{pmatrix}.
\]

\hint
Given any $n\times n$ matrix $A$, 
multiplying $A$ by the elementary matrix
$E_{i, j; \beta}$ {\it on the right\/} yields
the matrix $AE_{i, j; \beta}$ in which $\beta$ times the  $i$th column is
added to the $j$th column.



\vspace {0.25cm}\noindent
{\bf Problem B7 (30 pts).}
Given any two subspaces $V_1, V_2$ of a finite-dimensional
vector space $E$,
prove that
\begin{align*}
(V_1 + V_2)^{0} & = V_1^0\cap V_2^0 \\
(V_1 \cap V_2)^{0}& = V_1^0 +  V_2^0.
\end{align*}

Beware that in the second equation, $V_1$ and $V_2$ are subspaces of
$E$, not $E^*$.

\medskip\noindent
{\it Hint\/}. To prove the second equation, prove the inclusions
$V_1^0 +  V_2^0 \subseteq (V_1 \cap V_2)^{0}$ and
$(V_1 \cap V_2)^{0} \subseteq  V_1^0 +  V_2^0$.
Proving the second inclusion is a little tricky. First, prove that
we can pick a subspace $W_1$ of $V_1$ and a
subspace $W_2$ of $V_2$ such that 
\begin{enumerate}
\item
$V_1$ is the direct sum
$V_1 = (V_1\cap V_2) \oplus W_1$.
\item 
$V_2$ is the direct sum
$V_2 = (V_1\cap V_2) \oplus W_2$.
\item
$V_1 + V_2$ is the direct sum
$V_1 + V_2 = (V_1 \cap V_2) \oplus W_1\oplus W_2$.
\end{enumerate}



\vspace{0.5cm}\noindent
{\bf TOTAL: 340 + 20  points.}

\end{document}
